{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LDrm5tCFWGp4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip -q install openai\n",
        "!pip -q install langchain-openai\n",
        "!pip -q install langchain-core\n",
        "!pip -q install langchain-community\n",
        "!pip -q install sentence-transformers\n",
        "!pip -q install langchain-huggingface\n",
        "!pip -q install langchain_experimental\n",
        "!pip install gradio\n",
        "!pip install transformers\n",
        "!pip install easyocr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from getpass import getpass\n",
        "import easyocr\n",
        "\n",
        "import openai\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import google.colab.userdata as googlecolab"
      ],
      "metadata": {
        "id": "k0k5kPcnW2Ml"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = googlecolab.get('OA_API')\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "1eInJRamDn8Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_gpt4o = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "llm_gpt35 = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "llm_llama = ChatOpenAI(model_name=\"Llama-3.1-Nemotron-70B-Instruct-HF\", temperature=0)"
      ],
      "metadata": {
        "id": "md0vuOxtXFJ1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_conversation_history():\n",
        "    history = []\n",
        "    for message in memory.chat_memory.messages:\n",
        "      history.append(f\"{message.type}: {message.content}\\n\")\n",
        "    return \"\".join(set(history))\n",
        "\n",
        "def InvokeLLM(llm_choice, prompt):\n",
        "    prompt_template = PromptTemplate.from_template(\n",
        "        \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\n\\nHuman: {input}\")\n",
        "\n",
        "    if llm_choice == \"gpt-4o-mini\":\n",
        "        llm = llm_gpt4o\n",
        "    elif llm_choice == \"gpt-3.5-turbo\":\n",
        "        llm = llm_gpt35\n",
        "    elif llm_choice == \"Llama-3.1-Nemotron-70B-Instruct-HF\":\n",
        "        llm = llm_llama\n",
        "    else:\n",
        "        return \"Invalid LLM choice\"\n",
        "\n",
        "    conversation_chain = ConversationChain(\n",
        "        llm=llm,\n",
        "        prompt=prompt_template,\n",
        "        memory=memory,\n",
        "        verbose=False\n",
        "    )\n",
        "    response = conversation_chain.predict(input=prompt)\n",
        "    memory.save_context({\"input\": prompt}, {\"output\": response})\n",
        "    return response\n",
        "\n",
        "def combined_response(llm_choice, prompt, transcription, extraction):\n",
        "    if transcription != \"\":\n",
        "      transcript_text = transcription\n",
        "    else:\n",
        "      transcript_text = extraction\n",
        "\n",
        "    combined_prompt = f\"{prompt} Transcription: {transcript_text}\"\n",
        "    llm_response = InvokeLLM(llm_choice, combined_prompt)\n",
        "    return llm_response\n",
        "\n",
        "def clear_memory():\n",
        "  memory.clear()\n",
        "  return \" \"\n",
        "\n",
        "def transcribe(audio, speech_model):\n",
        "    transcriber = pipeline(\"automatic-speech-recognition\", model=speech_model)\n",
        "    try:\n",
        "      sr, y = audio\n",
        "      if y.ndim > 1:\n",
        "          y = y.mean(axis=1)\n",
        "      y = y.astype(np.float32)\n",
        "      y /= np.max(np.abs(y))\n",
        "    except Exception as e:\n",
        "      err = \"Error during transcription:  {0}\".format(e)\n",
        "      print(err)\n",
        "      return \"Transcription Error\"\n",
        "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
        "\n",
        "def ocr_read_image(image_path):\n",
        "    reader = easyocr.Reader(['en'])\n",
        "    result = reader.readtext(image_path)\n",
        "    text = \"\"\n",
        "    for detection in result:\n",
        "        text += detection[1] + \" \"\n",
        "    return text"
      ],
      "metadata": {
        "id": "OuoeTT4pGvEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------ Gradio development --------\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        llm_speech = gr.Radio([\"whisper-1\", \"openai/whisper-base.en\"], label=\"Choose Speeh-Text LLM: \", value=\"openai/whisper-base.en\")\n",
        "        audio_input = gr.Audio(sources=\"microphone\")\n",
        "        btn_transcribe = gr.Button(\"Generate Transcription\")\n",
        "        transcription = gr.Textbox(label=\"Transcription\")\n",
        "    with gr.Row():\n",
        "        image_input = gr.Image(type=\"filepath\")\n",
        "        btn_preview = gr.Button(\"Preview\") # Preview button\n",
        "        preview_output = gr.Image(label=\"Image Preview\") # Output for the preview\n",
        "        btn_extract_image = gr.Button(\"Extract Text from Image\")\n",
        "        extraction = gr.Textbox(label=\"Extracted Text\")\n",
        "    with gr.Row():\n",
        "        llm_choice = gr.Radio([\"gpt-4o-mini\", \"gpt-3.5-turbo\", \"Llama-3.1-Nemotron-70B-Instruct-HF\"], label=\"Choose Q&A LLM: \", value=\"gpt-3.5-turbo\")\n",
        "        prompt = gr.Textbox(label=\"Enter your prompt: \")\n",
        "    with gr.Row():\n",
        "        response = gr.Textbox(label=\"Response generated\")\n",
        "    memory = ConversationBufferMemory(return_messages=True)\n",
        "    btn_submit = gr.Button(\"Generate Response\")\n",
        "    btn_history = gr.Button(\"Show Conversation\")\n",
        "    btn_clear = gr.Button(\"Clear Memory\")\n",
        "\n",
        "    btn_transcribe.click(transcribe, inputs=[audio_input, llm_speech], outputs=transcription)\n",
        "    btn_preview.click(lambda x: x, inputs=image_input, outputs=preview_output) # Preview functionality\n",
        "    btn_extract_image.click(ocr_read_image, inputs=[image_input], outputs=extraction)\n",
        "    btn_submit.click(combined_response, inputs=[llm_choice, prompt, transcription, extraction], outputs=response)\n",
        "    btn_clear.click(clear_memory, outputs=response)\n",
        "    btn_history.click(get_conversation_history, outputs=response)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "KjQp5aAsZ5JR",
        "outputId": "62f7871f-1fe4-4e42-f233-c3a46a777718"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-59c86e5e5d10>:83: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(return_messages=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://eb1567f6ea835c9d45.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eb1567f6ea835c9d45.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7cwe93ZkC3go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Testing purpose -----------------\n",
        "InvokeLLM(\"gpt-3.5-turbo\", \"what is the capital of Singapore?\")\n",
        "InvokeLLM(\"gpt-3.5-turbo\", \"where is India?\")\n",
        "get_conversation_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "olyNteOX-m7I",
        "outputId": "3f804b4a-77a2-4182-817c-9556c40ab056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ai: AI: The capital of Singapore is Singapore City. It is a bustling metropolis known for its modern architecture, diverse culture, and delicious food. The city-state is located at the southern tip of the Malay Peninsula in Southeast Asia. It has a population of around 5.7 million people and is a major financial hub in the region. Is there anything else you would like to know about Singapore?\\nhuman: where is India?\\nhuman: what is the capital of Singapore?\\nai: AI: India is a country located in South Asia. It is known for its rich history, diverse culture, and vibrant traditions. India shares its borders with several countries including Pakistan, China, Nepal, Bhutan, Bangladesh, and Myanmar. The country is also bordered by the Indian Ocean to the south. With a population of over 1.3 billion people, India is the second most populous country in the world. Is there anything else you would like to know about India?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    }
  ]
}